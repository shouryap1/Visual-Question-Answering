{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset, set_caching_enabled\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    AutoModel,\n",
    ")\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "import subprocess\n",
    "\n",
    "# Download and unzip wordnet\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/kaggle/input/vqa-modified/VQA_modified/data.csv')\n",
    "answer_space=np.unique(np.array(df['answer'],dtype='str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_space=[i for i in answer_space]\n",
    "answer_space.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from CSV files for training and testing\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files={\n",
    "        \"train\": \"/kaggle/input/vqa-modified/VQA_modified/data.csv\",\n",
    "#         \"train\" : \"/kaggle/working/data_train.csv\",\n",
    "        \"test\": \"/kaggle/working/data_test.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# label each item in the dataset with their respective answers\n",
    "dataset = dataset.map(\n",
    "    lambda examples: {\n",
    "        'label': [\n",
    "            answer_space.index(ans)\n",
    "            for ans in examples['answer']\n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "def show_example(train=True, idx=None):\n",
    "    dataset_type = \"train\" if train else \"test\"\n",
    "    data = dataset[dataset_type]\n",
    "\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(data))\n",
    "\n",
    "    image_path =  \"/kaggle/input/vqa-modified/VQA_modified/images/\"+ f\"{data[idx]['image_id']}.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "    display.display(image)\n",
    "\n",
    "    question = data[idx][\"question\"]\n",
    "    answer = data[idx][\"answer\"]\n",
    "    label = data[idx][\"label\"]\n",
    "\n",
    "    print(f\"Question:\\t {question}\")\n",
    "    print(f\"Answer:\\t\\t {answer} (Label: {label})\")\n",
    "\n",
    "    return answer\n",
    "show_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAnswers(ids):\n",
    "    print([answer_space[id] for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int = len(answer_space),\n",
    "        intermediate_dim: int = 512,\n",
    "        pretrained_text_name: str = 'bert-base-uncased',\n",
    "        pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'\n",
    "    ):\n",
    "\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "        \n",
    "        # Text and image encoders\n",
    "        \n",
    "        self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
    "        \n",
    "        # Fusion module\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "\n",
    "        # Encode text with masking\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # Encode images\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # Combine encoded texts and images\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        logits = self.classifier(fused_output)\n",
    "        \n",
    "        out = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_vqa_collator_and_model(text_encoder='bert-base-uncased', image_encoder='google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "    # Initialize tokenizer and feature extractor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_encoder)\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(image_encoder)\n",
    "    \n",
    "    # Create Multimodal Collator\n",
    "    multimodal_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    # Create Multimodal VQA Model\n",
    "    multimodal_model = MultimodalVQAModel(\n",
    "        pretrained_text_name=text_encoder,\n",
    "        pretrained_image_name=image_encoder\n",
    "    ).to(device)\n",
    "\n",
    "    return multimodal_collator, multimodal_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model, save_model\n",
    "model_loaded = MultimodalVQAModel()\n",
    "load_model(model_loaded, '/kaggle/working/checkpoint/bert_vit/checkpoint-17200/model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator,model=create_multimodal_vqa_collator_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = \"/kaggle/working/checkpoint/bert_vit/\"\n",
    "\n",
    "\n",
    "# Get a list of all subdirectories in the checkpoint folder\n",
    "all_subdirectories = [d for d in os.listdir(checkpoint_folder) if os.path.isdir(os.path.join(checkpoint_folder, d))]\n",
    "\n",
    "# Filter only subdirectories starting with \"checkpoint-\"\n",
    "checkpoint_subdirectories = [d for d in all_subdirectories if d.startswith(\"checkpoint-\")]\n",
    "\n",
    "# Extract the checkpoint numbers from the subdirectory names\n",
    "checkpoint_numbers = [int(d.split(\"-\")[1]) for d in checkpoint_subdirectories]\n",
    "\n",
    "# Find the latest checkpoint number\n",
    "latest_checkpoint_number = max(checkpoint_numbers, default=0)\n",
    "\n",
    "# Construct the path for the latest checkpoint\n",
    "latest_checkpoint_path = os.path.join(checkpoint_folder, f\"checkpoint-{latest_checkpoint_number}/model.safetensors\")\n",
    "\n",
    "print(\"Latest Checkpoint Number:\", latest_checkpoint_number)\n",
    "print(\"Latest Checkpoint Path:\", latest_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly sample 10 indices from the test dataset\n",
    "random_sample_indices = random.sample(range(len(dataset[\"test\"])), k=30)\n",
    "\n",
    "# Sample data for manual testing\n",
    "sample = collator([dataset[\"test\"][index] for index in random_sample_indices])\n",
    "\n",
    "# Extract input components from the sample for manual testing\n",
    "input_ids = sample[\"input_ids\"].to(device)\n",
    "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "attention_mask = sample[\"attention_mask\"].to(device)\n",
    "pixel_values = sample[\"pixel_values\"].to(device)\n",
    "labels = sample[\"labels\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model_loaded.eval()\n",
    "\n",
    "# Forward pass with the sample data\n",
    "output = model_loaded(input_ids, pixel_values, attention_mask, token_type_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model output\n",
    "predictions = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def similarity(a, b):\n",
    "    # Split words if it is a list and remove extra spaces\n",
    "    words_a = [w.strip() for w in a.split(',')]\n",
    "    words_b = [w.strip() for w in b.split(',')]\n",
    "\n",
    "    # Split words if connected by underscore _\n",
    "    a = [w_ for word in words_a for w_ in word.split('_')]\n",
    "    b = [w_ for word in words_b for w_ in word.split('_')]\n",
    "\n",
    "    res = 0\n",
    "    n = 0\n",
    "\n",
    "    # Calculate score and take average\n",
    "    for i in a:\n",
    "        synsets_i = wordnet.synsets(i)\n",
    "        if synsets_i:\n",
    "            s1 = synsets_i[0]\n",
    "            for j in b:\n",
    "                synsets_j = wordnet.synsets(j)\n",
    "                if synsets_j:\n",
    "                    s2 = synsets_j[0]\n",
    "                    sim = s1.wup_similarity(s2)\n",
    "                    if sim:\n",
    "                        res += sim\n",
    "                    n += 1\n",
    "\n",
    "    return res / n if n != 0 else 0\n",
    "\n",
    "# Show predictions for a range of examples\n",
    "for i in range(0, 25):\n",
    "    print(\"\\n=========================================================\\n\")\n",
    "    real_answer = show_example(train=False, idx=i)\n",
    "    predicted_answer = answer_space[predictions[i]]\n",
    "    print(\"Predicted Answer:\\t\", predicted_answer)\n",
    "    print(f\"Similarity: {similarity(real_answer, predicted_answer)}\")\n",
    "    print(\"\\n=========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of trainable parameters: {:,}\".format(num_params))\n",
    "count_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
