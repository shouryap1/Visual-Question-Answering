{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers nltk numpy datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset, set_caching_enabled\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    # Preprocessing / Common\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
    "    AutoModel, AutoConfig,            \n",
    "    # Training / Evaluation\n",
    "    TrainingArguments, Trainer,\n",
    "    # Misc\n",
    "    logging\n",
    ")\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "import subprocess\n",
    "\n",
    "# Download and unzip wordnet\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache for Hugging Face Transformers and Datasets.\n",
    "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
    "\n",
    "set_caching_enabled(True)\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/kaggle/input/vqa-modified/VQA_modified/data.csv')\n",
    "answer_space=np.unique(np.array(df['answer'],dtype='str'))\n",
    "df_new=df[:10000]\n",
    "df_train=df[200:1000]\n",
    "df_new.to_csv('data_test.csv',index=False)\n",
    "df_train.to_csv('data_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_space=[i for i in answer_space]\n",
    "answer_space.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from CSV files for training and testing\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files={\n",
    "        \"train\": \"/kaggle/input/vqa-modified/VQA_modified/data.csv\",\n",
    "        \"test\": \"/kaggle/working/data_test.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda examples: {\n",
    "        'label': [\n",
    "            answer_space.index(ans)\n",
    "            for ans in examples['answer']\n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoFeatureExtractor\n",
    "    \n",
    "    def tokenize_text(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "    \n",
    "    def preprocess_images(self, images: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[\n",
    "                Image.open(os.path.join(\"/kaggle/input/vqa-modified/VQA_modified/images/\", f\"{image_id}.jpg\")).convert('RGB')\n",
    "                for image_id in images\n",
    "            ],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "            \n",
    "    def __call__(self, raw_batch_dict) -> Dict[str, torch.Tensor]:\n",
    "        question_batch = raw_batch_dict['question'] if isinstance(raw_batch_dict, dict) else [i['question'] for i in raw_batch_dict]\n",
    "        image_id_batch = raw_batch_dict['image_id'] if isinstance(raw_batch_dict, dict) else [i['image_id'] for i in raw_batch_dict]\n",
    "        label_batch = raw_batch_dict['label'] if isinstance(raw_batch_dict, dict) else [i['label'] for i in raw_batch_dict]\n",
    "\n",
    "        return {\n",
    "            **self.tokenize_text(question_batch),\n",
    "            **self.preprocess_images(image_id_batch),\n",
    "            'labels': torch.tensor(label_batch, dtype=torch.int64),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int = len(answer_space),\n",
    "        intermediate_dim: int = 512,\n",
    "        pretrained_text_name: str = 'bert-base-uncased',\n",
    "        pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'\n",
    "    ):\n",
    "        \n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "        \n",
    "        # Text and image encoders\n",
    "        \n",
    "        self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
    "        \n",
    "        # Fusion module\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "        \n",
    "        # Encode text with masking\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # Encode images\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # Combine encoded texts and images\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        logits = self.classifier(fused_output)\n",
    "        \n",
    "        out = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_vqa_collator_and_model(text_encoder='bert-base-uncased', image_encoder='google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "    # Initialize tokenizer and feature extractor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_encoder)\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(image_encoder)\n",
    "    \n",
    "    # Create Multimodal Collator\n",
    "    multimodal_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    # Create Multimodal VQA Model\n",
    "    multimodal_model = MultimodalVQAModel(\n",
    "        pretrained_text_name=text_encoder,\n",
    "        pretrained_image_name=image_encoder\n",
    "    ).to(device)\n",
    "\n",
    "    return multimodal_collator, multimodal_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wup_measure(a, b, similarity_threshold=0.925):\n",
    "\n",
    "    def get_semantic_field(word):\n",
    "        weight = 1.0\n",
    "        semantic_field = wordnet.synsets(word, pos=wordnet.NOUN)\n",
    "        return semantic_field, weight\n",
    "\n",
    "    def get_stem_word(word):\n",
    "        weight = 1.0\n",
    "        return word, weight\n",
    "\n",
    "    global_weight = 1.0\n",
    "\n",
    "    # Get stem words and weights\n",
    "    a, global_weight_a = get_stem_word(a)\n",
    "    b, global_weight_b = get_stem_word(b)\n",
    "    global_weight = min(global_weight_a, global_weight_b)\n",
    "\n",
    "    # Check if words are the same\n",
    "    if a == b:\n",
    "        return 1.0 * global_weight\n",
    "\n",
    "    # Check for empty strings\n",
    "    if a == \"\" or b == \"\":\n",
    "        return 0\n",
    "\n",
    "    # Get semantic fields and weights\n",
    "    interp_a, weight_a = get_semantic_field(a)\n",
    "    interp_b, weight_b = get_semantic_field(b)\n",
    "\n",
    "    # Check for empty semantic fields\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "\n",
    "    # Find the most optimistic interpretation\n",
    "    global_max = 0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score = x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max = local_score\n",
    "\n",
    "    # Use semantic fields and downweight unless the score is high (indicating synonyms)\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score = global_max * weight_a * weight_b * interp_weight * global_weight\n",
    "    return final_score\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "\n",
    "    # Returns:Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n",
    "\n",
    "    logits, labels = eval_tuple\n",
    "\n",
    "    # Calculate predictions\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for the model\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/checkpoint/\",            # Output directory for checkpoints and logs=\n",
    "    seed=12345,                         # Seed for reproducibility\n",
    "    evaluation_strategy=\"steps\",        # Evaluation strategy: \"steps\" or \"epoch\"\n",
    "    eval_steps=100,                     # Evaluate every 100 steps\n",
    "    logging_strategy=\"steps\",           # Logging strategy: \"steps\" or \"epoch\"\n",
    "    logging_steps=100,                  # Log every 100 steps\n",
    "    save_strategy=\"steps\",              # Saving strategy: \"steps\" or \"epoch\"\n",
    "    save_steps=100,                     # Save every 100 steps\n",
    "    save_total_limit=3,                 # Save only the last 3 checkpoints at any given time during training \n",
    "    metric_for_best_model='wups',       # Metric used for determining the best model\n",
    "    per_device_train_batch_size=32,     # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=32,      # Batch size per GPU for evaluation\n",
    "    remove_unused_columns=False,        # Whether to remove unused columns in the dataset\n",
    "    num_train_epochs=10,                 # Number of training epochs\n",
    "    fp16=True,                          # Enable mixed precision training (float16)\n",
    "    dataloader_num_workers=8,           # Number of workers for data loading\n",
    "    load_best_model_at_end=True,        # Whether to load the best model at the end of training\n",
    ")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_and_train_model(dataset, args, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224-in21k', multimodal_model='bert_vit'):\n",
    "\n",
    "    collator, model = create_multimodal_vqa_collator_and_model(text_model, image_model)\n",
    "    \n",
    "    multi_args = deepcopy(args)\n",
    "    multi_args.output_dir = os.path.join(\"/kaggle/working/checkpoint/\", multimodal_model)\n",
    "    print(multi_args.output_dir)\n",
    "\n",
    "    multi_trainer = Trainer(\n",
    "        model,\n",
    "        multi_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate for metrics\n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "    \n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics, multi_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics, trainer = create_and_train_model(dataset, args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
